{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The key features of TensorFlow\n",
    "## Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Creating a graph in TensorFlow v1.x\n",
    "- In earlier version of the TensorFlow low-level API, this graph had to be explicitly declared\n",
    "- In TensorFlow v1.x, a session is an environment in which the operations and tensors of a graph can be executed\n",
    "- After launching a graph in a TensorFlow session, we can execute its nodes, that is, evaluate its tensors or execute its operators\n",
    "- When evaluating a specific tensor in the graph, TensorFlow has to execute all the preceding nodes in the graph until it reaches the given node of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Result: z = 1\nResult: z = 1\n"
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.constant(1, name='a')\n",
    "    b = tf.constant(2, name='b')\n",
    "    c = tf.constant(3, name='c')\n",
    "    z = 2*(a - b) + c\n",
    "    \n",
    "with tf.compat.v1.Session(graph=g) as sess:\n",
    "    print('Result: z =', sess.run(z))\n",
    "    print('Result: z =', z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Migrating a graph to TensorFlow v2\n",
    "- TensorFlow v2 uses dynamic (as oposed to static) graphs by default (this is also called eager execution in TensorFlow), which allows us to evaluate an operation on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Result: z = 1\n"
    }
   ],
   "source": [
    "a = tf.constant(1, name='a')\n",
    "b = tf.constant(2, name='b')\n",
    "c = tf.constant(3, name='c')\n",
    "\n",
    "z = 2*(a - b) + c\n",
    "tf.print('Result: z =', z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Loading input data into a model: TensorFlow v1.x style\n",
    "- When using the TensorFlow v1.x low-level API, we had to create placeholder variables for providing input data to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Result: z = 1\n"
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.compat.v1.placeholder(shape=None, dtype=tf.int32, name='tf_a')\n",
    "    b = tf.compat.v1.placeholder(shape=None, dtype=tf.int32, name='tf_b')\n",
    "    c = tf.compat.v1.placeholder(shape=None, dtype=tf.int32, name='tf_c')\n",
    "    z = 2*(a - b) + c\n",
    "    \n",
    "with tf.compat.v1.Session(graph=g) as sess:\n",
    "    feed_dict = {a:1, b:2, c:3}\n",
    "    print('Result: z =', sess.run(z, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Loading input data into a model: TensorFlow v2 style\n",
    "- In TensorFlow v2, all this can simply be done by defining a regular Python function with a, b, and c as its input arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scalar Inputs: 1\nRank 1 Inputs: [1]\nRank 2 Inputs: [[1]]\n"
    }
   ],
   "source": [
    "def compute_z(a, b, c):\n",
    "    r1 = tf.subtract(a, b)\n",
    "    r2 = tf.multiply(2, r1)\n",
    "    z = tf.add(r2, c)\n",
    "    return z\n",
    "\n",
    "tf.print('Scalar Inputs:', compute_z(1, 2, 3))\n",
    "tf.print('Rank 1 Inputs:', compute_z([1], [2], [3]))\n",
    "tf.print('Rank 2 Inputs:', compute_z([[1]], [[2]], [[3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improving computational performance with function decorators\n",
    "- TensorFlow v2 provides a tool called AutoGraph that can automatically transform Python code into TensorFlow's graph code for faster execution\n",
    "- In addition, TensorFlow provides a simple mechanism for compiling a normal Python function to a static TensorFlow graph in order to make the computations more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scalar Inputs: 1\nRank 1 Inputs: [1]\nRank 2 Inputs: [[1]]\n"
    }
   ],
   "source": [
    "@tf.function\n",
    "def compute_z(a, b, c):\n",
    "    r1 = tf.subtract(a, b)\n",
    "    r2 = tf.multiply(2, r1)\n",
    "    z = tf.add(r2, c)\n",
    "    return z\n",
    "\n",
    "tf.print('Scalar Inputs:', compute_z(1, 2, 3))\n",
    "tf.print('Rank 1 Inputs:', compute_z([1], [2], [3]))\n",
    "tf.print('Rank 2 Inputs:', compute_z([[1]], [[2]], [[3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we want to want to limit the way a function can be called, we can specify its imput signature via a tuple of tf.TensorSpec objects when defining the function\n",
    "- Redefine the previous function, compute_z, and specify that only ran 1 tensors of type tf.int32 are allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 1 Inputs: [1]\nRank 1 Inputs: [1 2]\n"
    }
   ],
   "source": [
    "@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),\n",
    "                              tf.TensorSpec(shape=[None], dtype=tf.int32),\n",
    "                              tf.TensorSpec(shape=[None], dtype=tf.int32),))\n",
    "def compute_z(a, b, c):\n",
    "    r1 = tf.subtract(a, b)\n",
    "    r2 = tf.multiply(2, r1)\n",
    "    z = tf.add(r2, c)\n",
    "    return z\n",
    "\n",
    "tf.print('Rank 1 Inputs:', compute_z([1], [2], [3]))\n",
    "tf.print('Rank 1 Inputs:', compute_z([1, 2], [2, 4], [3, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## we expect this to result in an error\n",
    "tf.print('Rank 2 Inputs:', compute_z([[1], [2]], [[2], [4]], [[3], [6]]))\n",
    "\n",
    "\n",
    "## >> Error:\n",
    "#ValueError: Python inputs incompatible with input_signature: \n",
    "#inputs (([[1], [2]], [[2], [4]], [[3], [6]])), input_signature \n",
    "#((TensorSpec(shape=(None,), dtype=tf.int32, name=None), \n",
    "#  TensorSpec(shape=(None,), dtype=tf.int32, name=None), \n",
    "#  TensorSpec(shape=(None,), dtype=tf.int32, name=None)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorFlow Variable objects for storing and updating model parameters\n",
    "- In the context of TensorFlow, a Variable is a special Tensor object that allows us to store and update the parameters of our models during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tf.Variable 'var_a:0' shape=() dtype=float32, numpy=3.14>\n<tf.Variable 'var_b:0' shape=(3,) dtype=int32, numpy=array([1, 2, 3])>\n<tf.Variable 'Variable:0' shape=(2,) dtype=bool, numpy=array([ True, False])>\n<tf.Variable 'Variable:0' shape=(1,) dtype=string, numpy=array([b'abc'], dtype=object)>\n"
    }
   ],
   "source": [
    "a = tf.Variable(initial_value=3.14, name='var_a')\n",
    "b = tf.Variable(initial_value=[1, 2, 3], name='var_b')\n",
    "c = tf.Variable(initial_value=[True, False], dtype=tf.bool)\n",
    "d = tf.Variable(initial_value=['abc'], dtype=tf.string)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Trainable and untrainable variables\n",
    "- Variables have an attribute called trainable, which, by default, is set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "a.trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can define a non-trainable Variable as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False\n"
    }
   ],
   "source": [
    "w = tf.Variable([1, 2, 3], trainable=False)\n",
    "\n",
    "print(w.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modifying the values of variables\n",
    "- The values of a Variable can be efficiently modified by running some operations such as .assign(), .assign_add() and related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tf.Variable 'UnreadVariable' shape=(3,) dtype=int32, numpy=array([3, 1, 4])>\ntf.Tensor([5 0 6], shape=(3,), dtype=int32)\n"
    }
   ],
   "source": [
    "print(w.assign([3, 1, 4], read_value=True))\n",
    "w.assign_add([2, -1, 2], read_value=False)\n",
    "\n",
    "print(w.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initializing variables with random values for learning\n",
    "- For NN models intializing model parameters with random weights is necessary to break the symmetry during backpropagation - otherwise, a multilayer NN would be no more useful than a single-layer NN like logistic regression\n",
    "- We can create a Variable with Glorot initialization, which is a classic random initialization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.722795904 1.01456821 0.251808226]\n"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "init = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "tf.print(init(shape=(3,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.28982234 -0.782292783 -0.0453658961]\n [0.960991383 -0.120003454 0.708528221]]\n"
    }
   ],
   "source": [
    "v = tf.Variable(init(shape=(2, 3)))\n",
    "tf.print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Putting variables in the context of a more practical use case\n",
    "- We can define a Variable inside the base tf.Module class and access all of them using attributes made available by subclassing the tf.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "All module variables:  [TensorShape([2, 3]), TensorShape([1, 2])]\nTrainable variable:    [TensorShape([2, 3])]\n"
    }
   ],
   "source": [
    "class MyModule(tf.Module):\n",
    "    def __init__(self):\n",
    "        init = tf.keras.initializers.GlorotNormal()\n",
    "        self.w1 = tf.Variable(init(shape=(2, 3)), trainable=True)\n",
    "        self.w2 = tf.Variable(init(shape=(1, 2)), trainable=False)\n",
    "                \n",
    "m = MyModule()\n",
    "print('All module variables: ', [v.shape for v in m.variables])\n",
    "print('Trainable variable:   ', [v.shape for v in\n",
    "                                 m.trainable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Variables with tf.function\n",
    "- TensorFlow does not allow the creation of a Variable inside a decorated function and, as a result, the following code will raise an error (for reason refer to the book)\n",
    "```python\n",
    "\n",
    "## this will produce an error\n",
    "## ==> you cannot create a varibale inside a\n",
    "##     decorated function\n",
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "    w = tf.Variable([1, 2, 3])\n",
    "\n",
    "f([1])\n",
    "\n",
    "## ==> results in error\n",
    "## ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "\n",
    "```\n",
    "- One way to avoid this problem is to define the Variable outside of the decorated function and use it inside the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "w = tf.Variable(tf.random.uniform((3, 3)))\n",
    "\n",
    "@tf.function\n",
    "def compute_z(x):    \n",
    "    return tf.matmul(w, x)\n",
    "\n",
    "x = tf.constant([[1], [2], [3]], dtype=tf.float32)\n",
    "tf.print(compute_z(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing gradients via automatic differentiation and GradientTape\n",
    "- TensorFlow supports automatic differentiation, which can be thought of as an implementation of the chain rule for computing gradients of nested fucntions\n",
    "- When we define a series of operations that results in some output or even intermediate tensors, TensorFlow provides a context for calculating gradients of these computed tensors with respect to its dependent nodes in the computation graph\n",
    "- In order to compute these gradients, we have to \"record\" the computations via tf.GradientTape\n",
    "### 1. Computing the gradients of the loss with respect to trainable variables\n",
    "- We place the computation of z and the loss withing the tf.GradientTape context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True True\ndL/dw :  -0.559999764\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(1.0)\n",
    "b = tf.Variable(0.5)\n",
    "print(w.trainable, b.trainable)\n",
    "\n",
    "x = tf.convert_to_tensor([1.4])\n",
    "y = tf.convert_to_tensor([2.1])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tf.add(tf.multiply(w, x), b)\n",
    "    loss = tf.reduce_sum(tf.square(y - z))\n",
    "\n",
    "dloss_dw = tape.gradient(loss, w)\n",
    "\n",
    "tf.print('dL/dw : ', dloss_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Verify the computed gradient\n",
    "- Since this is a very simple example, we can obtain the derivatives symbolically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.559999764]\n"
    }
   ],
   "source": [
    "tf.print(2*x * ((w*x + b) - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Computing gradients with respect to non-trainable tensors\n",
    "- tf.GradientTape automatically supports the gradients for trainable variables\n",
    "- However, for non-trainable variables and other Tensor objects, we need to add an additional modifications to the GradientTape called tape.watch() to monitor those as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dL/dx: [-0.399999857]\n"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    z = tf.add(tf.multiply(w, x), b)\n",
    "    loss = tf.square(y - z)\n",
    "\n",
    "dloss_dx = tape.gradient(loss, x)\n",
    "\n",
    "tf.print('dL/dx:', dloss_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Verify the computed gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.399999857]\n"
    }
   ],
   "source": [
    "tf.print(2*w * ((w*x + b) - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Keeping resources for multiple gradient computations\n",
    "- When we monitor the computations in the context of th.GradientTape, by default, the tape will keep the resources only for a single gradient computation\n",
    "- If we want to compute more than one gradient, we need to make the tape persistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dL/dw: -0.559999764\ndL/db: -0.399999857\n"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = tf.add(tf.multiply(w, x), b)\n",
    "    loss = tf.reduce_sum(tf.square(y - z))\n",
    "\n",
    "dloss_dw = tape.gradient(loss, w)\n",
    "dloss_db = tape.gradient(loss, b)\n",
    "\n",
    "tf.print('dL/dw:', dloss_dw)\n",
    "tf.print('dL/db:', dloss_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Updating variables\n",
    "- If we are computing gradients of a loss term with respect to the parameters of a model, we can define an optimizer and apply the gradients to optimize the model parameters using the tf.keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Updated w: 1.0056\nUpdated bias: 0.504\n"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "optimizer.apply_gradients(zip([dloss_dw, dloss_db], [w, b]))\n",
    "\n",
    "tf.print('Updated w:', w)\n",
    "tf.print('Updated bias:', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simplifying implementations of common architectures via the Keras API \n",
    "### 1. Building a model with two densely (fully) connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                multiple                  80        \n_________________________________________________________________\ndense_1 (Dense)              multiple                  544       \n=================================================================\nTotal params: 624\nTrainable params: 624\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "\n",
    "## late variable creation\n",
    "model.build(input_shape=(None, 4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Printing variables (or model parameters) of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dense/kernel:0       True (4, 16)\ndense/bias:0         True (16,)\ndense_1/kernel:0     True (16, 32)\ndense_1/bias:0       True (32,)\n"
    }
   ],
   "source": [
    "for v in model.variables:\n",
    "    print('{:20s}'.format(v.name), v.trainable, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Configuring layers\n",
    " * Keras Initializers `tf.keras.initializers`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers  \n",
    " * Keras Regularizers `tf.keras.regularizers`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/regularizers  \n",
    " * Activations `tf.keras.activations`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations  \n",
    "### 3.2. Configure the first layer by specifying initializers for the kernel and bias variables and the second layer by specifying an L1 regularizer for the kernel (weight matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              multiple                  80        \n_________________________________________________________________\ndense_3 (Dense)              multiple                  544       \n=================================================================\nTotal params: 624\nTrainable params: 624\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=16, \n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "        bias_initializer=tf.keras.initializers.Constant(2.0)\n",
    "    ))\n",
    "\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=32, \n",
    "        activation=tf.keras.activations.sigmoid,\n",
    "        kernel_regularizer=tf.keras.regularizers.l1\n",
    "    ))\n",
    "\n",
    "model.build(input_shape=(None, 4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Compiling a model\n",
    " * Keras Optimizers `tf.keras.optimizers`:  https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers\n",
    " * Keras Loss Functins `tf.keras.losses`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses\n",
    " * Keras Metrics `tf.keras.metrics`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics\n",
    "\n",
    "### 4.2 Compile the model using the SGD optimizer, cross-entropy loss for binary classification, and a specific list of metrics, including accuracy, precision, and recall\n",
    "- In addition to configuring the individual layers, we can also configure the model when we compile it\n",
    " - We can specify the type of optimizer and the loss function for training, as well as which metrics to use for reporting the performance on the training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.Accuracy(), \n",
    "             tf.keras.metrics.Precision(),\n",
    "             tf.keras.metrics.Recall(),])"
   ]
  }
 ]
}