{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Project two: character-level language modeling in TensorFlow\n",
    "## 1. Preprocessing the dataset\n",
    "### 1. Read the dataset as plain text\n",
    "- We also remove portions from the beginning and the end, since they are not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "567 1112917\nTotal Length: 1112350\nUnique Characters: 80\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Reading and processing text\n",
    "with open('data/1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "    \n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "print(start_indx, end_indx)\n",
    "\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the dictionary to map characters to integers\n",
    "- Reverse mapping is done via indexing a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text encoded shape:  (1112350,)\nTHE MYSTERIOUS       == Encoding ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n[33 43 36 25 38 28]  == Reverse  ==>  ISLAND\n"
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "\n",
    "text_encoded = np.array(\n",
    "    [char2int[ch] for ch in text],\n",
    "    dtype=np.int32)\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a TensorFlow dataset from this array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "44 -> T\n32 -> H\n29 -> E\n1 ->  \n37 -> M\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "\n",
    "for ex in ds_text_encoded.take(5):\n",
    "    print('{} -> {}'.format(ex.numpy(), char_array[ex.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Separate the input and target sequences accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\nTarget (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n\n Input (x): ' Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n'\nTarget (y): 'Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n\\n'\n\n"
    }
   ],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "ds_chunks = ds_text_encoded.batch(chunk_size, drop_remainder=True)\n",
    "\n",
    "## define the function for splitting x & y\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    return input_seq, target_seq\n",
    "\n",
    "ds_sequences = ds_chunks.map(split_input_target)\n",
    "\n",
    "## inspection:\n",
    "for example in ds_sequences.take(2):\n",
    "    print(' Input (x):', repr(''.join(char_array[example[0].numpy()])))\n",
    "    print('Target (y):', repr(''.join(char_array[example[1].numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Divide the dataset into mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<BatchDataset shapes: ((None, 40), (None, 40)), types: (tf.int32, tf.int32)>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)# drop_remainder=True)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a character-level RNN model\n",
    "### 1. Write a function that defines an RNN model using the Keras Sequential class, specify traininng parameters and obtain an RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, None, 256)         20480     \n_________________________________________________________________\nlstm (LSTM)                  (None, None, 512)         1574912   \n_________________________________________________________________\ndense (Dense)                (None, None, 80)          41040     \n=================================================================\nTotal params: 1,636,432\nTrainable params: 1,636,432\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "## Set the training parameters\n",
    "charset_size = len(char_array)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size = charset_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/20\n424/424 [==============================] - 299s 706ms/step - loss: 2.3011\nEpoch 2/20\n424/424 [==============================] - 287s 676ms/step - loss: 1.7332\nEpoch 3/20\n424/424 [==============================] - 295s 695ms/step - loss: 1.5343\nEpoch 4/20\n424/424 [==============================] - 300s 707ms/step - loss: 1.4204\nEpoch 5/20\n424/424 [==============================] - 304s 718ms/step - loss: 1.3477\nEpoch 6/20\n424/424 [==============================] - 318s 749ms/step - loss: 1.2976\nEpoch 7/20\n424/424 [==============================] - 315s 743ms/step - loss: 1.2597\nEpoch 8/20\n424/424 [==============================] - 280s 660ms/step - loss: 1.2286\nEpoch 9/20\n424/424 [==============================] - 281s 664ms/step - loss: 1.2030\nEpoch 10/20\n424/424 [==============================] - 279s 658ms/step - loss: 1.1817\nEpoch 11/20\n424/424 [==============================] - 280s 661ms/step - loss: 1.1618\nEpoch 12/20\n424/424 [==============================] - 281s 663ms/step - loss: 1.1446\nEpoch 13/20\n424/424 [==============================] - 283s 667ms/step - loss: 1.1283\nEpoch 14/20\n424/424 [==============================] - 282s 664ms/step - loss: 1.1127\nEpoch 15/20\n424/424 [==============================] - 270s 636ms/step - loss: 1.0985\nEpoch 16/20\n424/424 [==============================] - 265s 626ms/step - loss: 1.0846\nEpoch 17/20\n424/424 [==============================] - 266s 626ms/step - loss: 1.0719\nEpoch 18/20\n424/424 [==============================] - 265s 625ms/step - loss: 1.0589\nEpoch 19/20\n424/424 [==============================] - 267s 629ms/step - loss: 1.0460\nEpoch 20/20\n424/424 [==============================] - 279s 658ms/step - loss: 1.0337\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x220409d1d08>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True\n",
    "    ))\n",
    "\n",
    "model.fit(ds, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation phase: generating new text passages\n",
    "### 1. Drawing random samples from a categorical distribution\n",
    "- If we simply select the element with the highest logit value, the model will always produce the same text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Probabilities: [0.33333334 0.33333334 0.33333334]\narray([[0, 0, 1, 2, 0, 0, 0, 0, 1, 0]], dtype=int64)\n"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "logits = [[1.0, 1.0, 1.0]]\n",
    "print('Probabilities:', tf.math.softmax(logits).numpy()[0])\n",
    "\n",
    "samples = tf.random.categorical(\n",
    "    logits=logits, num_samples=10)\n",
    "tf.print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Probabilities: [0.10650698 0.10650698 0.78698605]\narray([[2, 0, 2, 2, 2, 0, 1, 2, 2, 0]], dtype=int64)\n"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "logits = [[1.0, 1.0, 3.0]]\n",
    "print('Probabilities:', tf.math.softmax(logits).numpy()[0])\n",
    "\n",
    "samples = tf.random.categorical(\n",
    "    logits=logits, num_samples=10)\n",
    "tf.print(samples.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the sampling function and generate some new text\n",
    "- The sampling function takes a string as an input, generates a new sequence of characters with a new predicted character in the end of it, then appends this new character to the end of the generated text string, and finally goes back to predicting the next character, but now using the new end of the text string of the input string length instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The island is or circies or of stone. These well of the radiant\nproved with turn to the highest bears questional.\n\nThere was no occasion for three day and stood with rocks before the pontrance was pursued from an immense elamners,\nwho had fallen into the island and might be gone against a sole harfless finished quait and all that and dry wanting together\nup on the rocks, whose six filled on this\nbeands of Red Creek Glycerine, and the sunning knelled to the beach, mulphy open. The lava made not all\nthree h\n"
    }
   ],
   "source": [
    "def sample(model, starting_str, \n",
    "           len_generated_text=500, \n",
    "           max_input_length=40,\n",
    "           scale_factor=1.0):\n",
    "    encoded_input = [char2int[s] for s in starting_str]\n",
    "    encoded_input = tf.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(len_generated_text):\n",
    "        logits = model(encoded_input)\n",
    "        logits = tf.squeeze(logits, 0)\n",
    "\n",
    "        scaled_logits = logits * scale_factor\n",
    "        new_char_indx = tf.random.categorical(\n",
    "            scaled_logits, num_samples=1)\n",
    "        \n",
    "        new_char_indx = tf.squeeze(new_char_indx)[-1].numpy()    \n",
    "\n",
    "        generated_str += str(char_array[new_char_indx])\n",
    "        \n",
    "        new_char_indx = tf.expand_dims([new_char_indx], 0)\n",
    "        encoded_input = tf.concat(\n",
    "            [encoded_input, new_char_indx],\n",
    "            axis=1)\n",
    "        encoded_input = encoded_input[:, -max_input_length:]\n",
    "\n",
    "    return generated_str\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The island'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predictability vs. randomness\n",
    "- Scaling the logits computed by the RNN model before passing them to tf.random.categorical() allows us to control the predictability of the generated samples (that is, generating text following the learned patterns from the training text versus adding more randomness)\n",
    "\n",
    "### 1. Demonstration of scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Probabilities before scaling:         [0.10650698 0.10650698 0.78698604]\nProbabilities after scaling with 0.5: [0.21194156 0.21194156 0.57611688]\nProbabilities after scaling with 0.1: [0.31042377 0.31042377 0.37915245]\n"
    }
   ],
   "source": [
    "logits = np.array([[1.0, 1.0, 3.0]])\n",
    "\n",
    "print('Probabilities before scaling:        ', tf.math.softmax(logits).numpy()[0])\n",
    "\n",
    "print('Probabilities after scaling with 0.5:', tf.math.softmax(0.5*logits).numpy()[0])\n",
    "\n",
    "print('Probabilities after scaling with 0.1:', tf.math.softmax(0.1*logits).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating texts with different scaling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The island was so as to discover the poultry-yard, and the heat was continued.\n\nThe colonists had had the convicts had not less than extreme surprise, and the truth had been discovered the colonists.\n\n“The will not be resistance the internal signal to the sea, and the convicts were already painful to the corral and the lad served as if the convicts would\nproduce a stern to the shore.\n\nThe first bark of the Mercy, and the flood of the other end of the water.\n\nThe reporter and his companions had already sti\n"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The island', \n",
    "             scale_factor=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The island\nhappilid a drems parts,\nwithlessly a? Taarif-sadcepe or valution. “Pellowd Cyrul or a lantern, adreps Spilett. Tollow-deeq riferachmve.\n\nNea?--low, whyn’,\nespecialas ockurarigish Harding,” observed\ndayend, tiorde-flammed. Lef Grant’ somhorsity is heard. The cossition immediftwappie-clemescops, domphiams braThted dash\nagains, who\nacquierhup” re up, Harding.\n thus wishint did not quitw.\n\n“Albordir.\n\nOn, doffully\nhoper, during vessel fleard!”\nreturned Gbantt qunsreg. Nothingly, I\nwill--77\n\nYet?” a\n"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The island', \n",
    "             scale_factor=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}